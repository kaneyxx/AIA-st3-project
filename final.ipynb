{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install torch==1.2.0  transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入需要使用的套件\n",
    "'''\n",
    "PyTorch相關內容\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset,DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "import numpy as np\n",
    "'''\n",
    "Bert模型來源\n",
    "'''\n",
    "from transformers import *\n",
    "'''\n",
    "評估指標\n",
    "'''\n",
    "from sklearn.metrics import f1_score\n",
    "'''\n",
    "訓練即時偵測/視覺化工具\n",
    "'''\n",
    "import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "'''\n",
    "其他相關套件\n",
    "'''\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方便重現結果,把所有可以random的都設定seed\n",
    "def manual_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列出現有的 GPU列表\n",
    "list(range(torch.cuda.device_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定要使用的 GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取預訓練好的 tokenizer及其 config以方便之後模型建構\n",
    "bio_tokenizer = BertTokenizer.from_pretrained(\"monologg/biobert_v1.1_pubmed\")\n",
    "bio_config = BertConfig.from_pretrained(\"monologg/biobert_v1.1_pubmed\", num_labels=5, finetuning_task=\"ddi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定我們要使用的 special tokens\n",
    "ADDITIONAL_SPECIAL_TOKENS = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n",
    "bio_tokenizer.add_special_tokens({\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從資料 label.txt中取得對應的label並轉成 list\n",
    "def get_label():\n",
    "    return [label.strip() for label in open(f\"./label.txt\", 'r', encoding='utf-8')]\n",
    "\n",
    "get_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers套件內建class物件, 可將單一data取出\n",
    "# 參考資訊： https://huggingface.co/transformers/main_classes/processors.html\n",
    "class InputExample(object):\n",
    "\n",
    "    def __init__(self, guid, text_a, label):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers套件內建class物件, 將input features做整合\n",
    "# 參考資訊同上\n",
    "class InputFeatures(object):\n",
    "    \n",
    "    def __init__(self, input_ids, attention_mask, token_type_ids, label_id, e1_mask, e2_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.label_id = label_id\n",
    "        self.e1_mask = e1_mask\n",
    "        self.e2_mask = e2_mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIProcessor(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.relation_labels = get_label() # 取得 labels\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        # 讀取 tsv檔\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        # 將 tsv讀取出的資料轉成之後要轉成 Features的格式\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)  # 取出 id\n",
    "            text_a = line[1]                # 取出文句\n",
    "            text_a = text_a.lower()         # 轉成小寫\n",
    "            label = self.relation_labels.index(line[0])\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, label=label))\n",
    "        return examples\n",
    "\n",
    "    def get_examples(self, mode):\n",
    "        file_to_read = None\n",
    "        if mode == 'train':\n",
    "            file_to_read = \"./train.tsv\"\n",
    "        elif mode == 'test':\n",
    "            file_to_read = \"./test.tsv\"\n",
    "        return self._create_examples(self._read_tsv(file_to_read), mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, max_seq_len, tokenizer,\n",
    "                                 cls_token_segment_id=0,\n",
    "                                 pad_token_segment_id=0,\n",
    "                                 sequence_a_segment_id=0,\n",
    "                                 mask_padding_with_zero=True):\n",
    "    # 預設的 special tokens\n",
    "    cls_token = tokenizer.cls_token        # [CLS]\n",
    "    sep_token = tokenizer.sep_token        # [SEP]\n",
    "    pad_token_id = tokenizer.pad_token_id  # [PAD]\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "\n",
    "        # 取出文句後利用 tokenizer先行斷句\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        e11_p = tokens_a.index(\"<e1>\")   # 第一種藥的起始\n",
    "        e12_p = tokens_a.index(\"</e1>\")  # 第一種藥的結尾\n",
    "        e21_p = tokens_a.index(\"<e2>\")   # 第二種藥的起始\n",
    "        e22_p = tokens_a.index(\"</e2>\")  # 第二種藥的結尾\n",
    "\n",
    "        # 將我們自定義的special token置換成vocab中已有的符號\n",
    "        tokens_a[e11_p] = \"$\"\n",
    "        tokens_a[e12_p] = \"$\"\n",
    "        tokens_a[e21_p] = \"#\"\n",
    "        tokens_a[e22_p] = \"#\"\n",
    "\n",
    "        # 先加1因為等下咱們句首會加上[CLS]\n",
    "        e11_p += 1\n",
    "        e12_p += 1\n",
    "        e21_p += 1\n",
    "        e22_p += 1\n",
    "\n",
    "        # 在句尾放上[SEP]\n",
    "        tokens = tokens_a\n",
    "        tokens += [sep_token]\n",
    "\n",
    "        # 標示序列為第幾句\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        # 句首加上[CLS]\n",
    "        tokens = [cls_token] + tokens\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "        \n",
    "        # 利用 tokenizer轉成vocab相對應的 id\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # attention mask中, 1為要做attention的tokens, 0則否\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # padding到自訂最大長度\n",
    "        padding_length = max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        # 自製 e1_mask, e2_mask\n",
    "        e1_mask = [0] * len(attention_mask)\n",
    "        e2_mask = [0] * len(attention_mask)\n",
    "\n",
    "        for i in range(e11_p, e12_p + 1):\n",
    "            e1_mask[i] = 1\n",
    "        for i in range(e21_p, e22_p + 1):\n",
    "            e2_mask[i] = 1\n",
    "\n",
    "        # 利用 assert確認一下轉換的序列長度避免出錯\n",
    "        assert len(input_ids) == max_seq_len, \"input_id 序列長度發生錯誤 {} vs 最大序列長度{}\".format(len(input_ids), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"attention mask 序列長度發生錯誤 {} vs 最大序列長度{}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_ids) == max_seq_len, \"token type id 序列長度發生錯誤 {} vs 最大序列長度{}\".format(len(token_type_ids), max_seq_len)\n",
    "        assert len(e1_mask) == max_seq_len, \"e1_mask 序列長度發生錯誤 {} vs 最大序列長度{}\".format(len(token_type_ids), max_seq_len)\n",
    "        assert len(e2_mask) == max_seq_len, \"e2_mask 序列長度發生錯誤 {} vs 最大序列長度{}\".format(len(token_type_ids), max_seq_len)\n",
    "\n",
    "        label_id = int(example.label)\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids,\n",
    "                          label_id=label_id,\n",
    "                          e1_mask=e1_mask,\n",
    "                          e2_mask=e2_mask))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(tokenizer, max_len, mode):\n",
    "    processor = DDIProcessor()\n",
    "    \n",
    "    # 建立快取檔案(若已有建立則直接讀取)\n",
    "    cached_file_name = 'bert_ddi_cached_{}_{}'.format(max_len, mode)\n",
    "    cached_file_name = cached_file_name + '_lower'\n",
    "\n",
    "    cached_features_file = \"./\" + cached_file_name\n",
    "    \n",
    "    if os.path.exists(cached_features_file):\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        if mode == \"train\":\n",
    "            examples = processor.get_examples(\"train\")\n",
    "        elif mode == \"test\":\n",
    "            examples = processor.get_examples(\"test\")\n",
    "        else:\n",
    "            raise Exception(\"mode只能輸入train或test\")\n",
    "\n",
    "        features = convert_examples_to_features(examples, max_len, tokenizer)\n",
    "        torch.save(features, cached_features_file) # 儲存快取\n",
    "\n",
    "    # 轉成 PyTorch使用的張量\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    all_e1_mask = torch.tensor([f.e1_mask for f in features], dtype=torch.long)\n",
    "    all_e2_mask = torch.tensor([f.e2_mask for f in features], dtype=torch.long)\n",
    "\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    \n",
    "    # 轉成張量資料集\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask,\n",
    "                            all_token_type_ids, all_label_ids, all_e1_mask, all_e2_mask)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立訓練資料集及快取檔案\n",
    "train_set = load_and_cache_examples(bio_tokenizer, 300, \"train\")\n",
    "\n",
    "# 建立測試資料集及快取檔案\n",
    "test_set = load_and_cache_examples(bio_tokenizer, 300, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將訓練資料集分割出驗證資料集 (原訓練資料總數為12841)\n",
    "train_set, eval_set = random_split(train_set, [9600, 3241])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 batch size\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定讀取訓練資料集的形式\n",
    "train_sampler = RandomSampler(train_set)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 設定讀取驗證資料集的形式\n",
    "eval_sampler = SequentialSampler(eval_set)\n",
    "\n",
    "eval_dataloader = DataLoader(train_set, sampler=eval_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 設定讀取測試資料集的形式\n",
    "test_sampler = SequentialSampler(test_set)\n",
    "\n",
    "test_dataloader = DataLoader(test_set, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建構自定義全連接層\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0., use_activation=True):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.use_activation = use_activation\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        if self.use_activation:\n",
    "            x = self.tanh(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建構模型\n",
    "class BERT(BertPreTrainedModel):\n",
    "    def __init__(self, bert_config):\n",
    "        super(BERT, self).__init__(bert_config)\n",
    "        # 使用已讀取的config 載入預訓練的 BioBERT模型 \n",
    "        self.bert = BertModel.from_pretrained(\"monologg/biobert_v1.1_pubmed\", config=bert_config)\n",
    "        self.num_labels = bert_config.num_labels  # 指定分類的類別數\n",
    "        self.cls_fc_layer = FCLayer(768, 768, dropout_rate=0.1)\n",
    "        self.e1_fc_layer = FCLayer(768, 768, dropout_rate=0.1)\n",
    "        self.e2_fc_layer = FCLayer(768, 768, dropout_rate=0.1)\n",
    "        self.label_classifier = FCLayer(768 * 3, bert_config.num_labels, dropout_rate=0.1, use_activation=False)\n",
    "\n",
    "    # 設定計算抽取的entity vectors\n",
    "    @staticmethod\n",
    "    def entity_average(output, e_mask):\n",
    "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # shape為 [batch_size, 1, j-i+1]\n",
    "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [b, dim]\n",
    "\n",
    "        # .bmm 為 batch matrix multiply\n",
    "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), output).squeeze(1)  # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n",
    "        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\n",
    "        return avg_vector\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels, e1_mask, e2_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]  # [CLS]\n",
    "\n",
    "        # Average\n",
    "        e1_h = self.entity_average(sequence_output, e1_mask)\n",
    "        e2_h = self.entity_average(sequence_output, e2_mask)\n",
    "\n",
    "        # Dropout -> tanh -> fc_layer\n",
    "        pooled_output = self.cls_fc_layer(pooled_output)\n",
    "        e1_h = self.e1_fc_layer(e1_h)\n",
    "        e2_h = self.e2_fc_layer(e2_h)\n",
    "\n",
    "        # Concat -> label_classifier\n",
    "        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-1)\n",
    "        logits = self.label_classifier(concat_h)\n",
    "        \n",
    "        # 計算 loss\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                loss_fct = nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                \n",
    "        outputs = (loss, logits)\n",
    "\n",
    "        return outputs  # loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型呼叫建構並分配給 GPU\n",
    "model = BERT(bio_config)\n",
    "model.cuda()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定訓練 epoch次數\n",
    "EPOCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 optimizer和 scheduler (linear warmup and decay)\n",
    "# 參考資訊：https://huggingface.co/transformers/main_classes/optimizer_schedules.html\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=600,   # 熱身步數(熱身期間會使 lr逐漸增加到1)\n",
    "                                            num_training_steps=len(train_set) * EPOCH / BATCH_SIZE,  # 9600 * 5 / 16\n",
    "                                            last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義下我們要評估模型的指標: accuracy, F1 score\n",
    "def acc_and_f1(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    acc = (preds == labels).mean()\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, labels=[1, 2, 3, 4], average='micro')\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定模型儲存路徑\n",
    "PATH = \"./model-final.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "     訓練\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "global_step = 0           # 總步數\n",
    "tr_loss = 0.0             # total loss\n",
    "writer = SummaryWriter()  # tensorboard紀錄用\n",
    "best_acc = 0.0            # 評估時紀錄目前最好的 accuracy\n",
    "best_f1 = 0.0             # 評估時紀錄目前最好的 f1 score\n",
    "\n",
    "for epoch_i in range(EPOCH):\n",
    "    print(\"\")\n",
    "    print(f'======== Epoch {epoch_i + 1} / {EPOCH} ========')\n",
    "    print('訓練模型...')\n",
    "    \n",
    "    iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "    \n",
    "    steps_in_epoch = 0 # epoch中的步數\n",
    "    \n",
    "    for step, batch in enumerate(iterator):\n",
    "        # 將模型設置為訓練模式\n",
    "        model.train()\n",
    "        \n",
    "        # 模型清空梯度\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 將 batch中資料配置到 GPU\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        \n",
    "        # 包含六種不同的PyTorch張量:\n",
    "        # [0]: input_ids \n",
    "        # [1]: token_type_ids - 此用來區別前後語句, 此專題不需要\n",
    "        # [2]: attention_masks\n",
    "        # [3]: labels \n",
    "        # [4]: e1_mask\n",
    "        # [5]: e2_mask\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids': batch[2],\n",
    "                  'labels': batch[3],\n",
    "                  'e1_mask': batch[4],\n",
    "                  'e2_mask': batch[5]}\n",
    "        \n",
    "        # 過模型 feed-forward pass, 將 loss取出\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        # Tensorbaord紀錄 loss\n",
    "        writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "        \n",
    "        # backpropagation計算梯度\n",
    "        loss.backward()\n",
    "        \n",
    "        # 儲存 total loss\n",
    "        tr_loss += loss.item()\n",
    "        \n",
    "        # 設置 clip值避免 exploding\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 更新 weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 更新 learning rate(若沒設定scheduler則忽略)\n",
    "        scheduler.step()\n",
    "\n",
    "        # 更新步數\n",
    "        steps_in_epoch += 1\n",
    "        global_step += 1\n",
    "        \n",
    "        # 訓練一部分後即進行評估\n",
    "        if steps_in_epoch % 300 == 0:\n",
    "            \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                 評估\n",
    "            \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            print(f'評估模型 {steps_in_epoch // 300}/2')\n",
    "            eval_loss = 0.0        # 紀錄 evaluation loss\n",
    "            eval_steps = 0         # 紀錄 evaluation steps\n",
    "            preds = None           # 紀錄預測的結果\n",
    "            out_label_ids = None   # 紀錄 ground truth (labels)\n",
    "\n",
    "            # 將模型設為評估模式\n",
    "            model.eval()\n",
    "\n",
    "            for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "                batch = tuple(t.cuda() for t in batch)\n",
    "                # 不需要計算梯度\n",
    "                with torch.no_grad():\n",
    "                    inputs = {'input_ids': batch[0],\n",
    "                              'attention_mask': batch[1],\n",
    "                              'token_type_ids': batch[2],\n",
    "                              'labels': batch[3],\n",
    "                              'e1_mask': batch[4],\n",
    "                              'e2_mask': batch[5]}\n",
    "                    outputs = model(**inputs)\n",
    "                    tmp_eval_loss, logits = outputs\n",
    "\n",
    "                    eval_loss += tmp_eval_loss.mean().item()\n",
    "                eval_steps += 1\n",
    "\n",
    "                if preds is None:\n",
    "                    preds = logits.detach().cpu().numpy()\n",
    "                    out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "                else:\n",
    "                    preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "                    out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "            # 計算平均測試 loss\n",
    "            eval_loss = eval_loss / eval_steps\n",
    "            # 利用 argmax取出預測的分類\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "            # 計算 accuracy / f1 score\n",
    "            acc, f1 = acc_and_f1(preds, out_label_ids)\n",
    "            \n",
    "            # Tensorboard紀錄評估結果\n",
    "            writer.add_scalar('Valid/loss', eval_loss, global_step)\n",
    "            writer.add_scalar('Valid/acc', acc, global_step)\n",
    "            writer.add_scalar('Valid/F1', f1, global_step)\n",
    "\n",
    "            # 更新 best_acc, best_f1, 如果進步便儲存更新模型\n",
    "            if acc >= best_acc and f1 >= best_f1:\n",
    "                best_acc = acc\n",
    "                best_f1 = f1\n",
    "                # 儲存模型(此儲存方式為PyTorch官方建議方法, 只儲存參數)\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "\n",
    "                print(f\"\"\"\n",
    "                模型更新！ 目前已訓練{global_step}筆batch資料\n",
    "                評估結果為:\n",
    "                ---------------------\n",
    "                {\"loss\"}: {eval_loss:.6f} \n",
    "                {\"acc\"} : {best_acc:.6f}\n",
    "                {\"F1\"}   : {best_f1:.6f}\n",
    "                ---------------------\n",
    "                \"\"\")\n",
    "            else:\n",
    "                print(\"模型未更新！\")\n",
    "\n",
    "# 關閉 Tensorboard寫入\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取模型\n",
    "model = BERT(bio_config).cuda()\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    最終評估測試集\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "test_loss = 0.0        # evaluation loss\n",
    "test_steps = 0         # evaluation steps\n",
    "preds = None           # 紀錄預測的結果\n",
    "out_label_ids = None   # 紀錄ground truth (labels)\n",
    "\n",
    "# 將模型設為評估模式\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "    batch = tuple(t.cuda() for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids': batch[2],\n",
    "                  'labels': batch[3],\n",
    "                  'e1_mask': batch[4],\n",
    "                  'e2_mask': batch[5]}\n",
    "        outputs = model(**inputs)\n",
    "        tmp_test_loss, logits = outputs[:2]\n",
    "\n",
    "        test_loss += tmp_test_loss.mean().item()\n",
    "    test_steps += 1\n",
    "\n",
    "    if preds is None:\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "    else:\n",
    "        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "        out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "# 計算平均 loss\n",
    "final_loss = test_loss / test_steps\n",
    "# 利用 argmax取出預測的分類\n",
    "preds = np.argmax(preds, axis=1)\n",
    "# 計算 accuracy / f1 score\n",
    "acc, f1 = acc_and_f1(preds, out_label_ids)\n",
    "\n",
    "print(f\"\"\"\n",
    "最終測試結果:\n",
    "---------------------\n",
    "{\"loss\"}: {final_loss:.6f} \n",
    "{\"acc\"} : {acc:.6f}\n",
    "{\"F1\"}   : {f1:.6f}\n",
    "---------------------\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
